{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Stream Analysis Flow\n",
    "This notebook processes YouTube transcripts and live chat logs to analyze engagement, spikes, and keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Workspace Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import textwrap\n",
    "\n",
    "# Custom module\n",
    "import parsers\n",
    "import youtube_client # Ensure this file exists in your directory\n",
    "\n",
    "# Pre-download NLTK data\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Chart styling\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTIONS ---\n",
    "def extract_transcript_by_minutes(transcript_df, minutes):\n",
    "    \"\"\"\n",
    "    Retrieves subtitle text for specific minutes.\n",
    "    \n",
    "    Args:\n",
    "        transcript_df (pd.DataFrame): The dataframe containing transcript data.\n",
    "        minutes (list or int): A single minute (int) or list of minutes to retrieve.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where key is the minute (int) and value is the text (str).\n",
    "    \"\"\"\n",
    "    if isinstance(minutes, (int, float)):\n",
    "        minutes = [int(minutes)]\n",
    "        \n",
    "    results = {}\n",
    "    for m in minutes:\n",
    "        start_sec = m * 60\n",
    "        end_sec = (m + 1) * 60\n",
    "        \n",
    "        # Filter rows that start within this minute\n",
    "        mask = (transcript_df['offset_start_seconds'] >= start_sec) & \\\n",
    "               (transcript_df['offset_start_seconds'] < end_sec)\n",
    "               \n",
    "        segment = transcript_df[mask]\n",
    "        if not segment.empty:\n",
    "            # Join text and clean up extra spaces\n",
    "            text = \" \".join(segment['text'].tolist())\n",
    "            results[m] = text.strip()\n",
    "        else:\n",
    "            results[m] = \"(No speech detected)\"\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YOUTUBE_URL = \"https://www.youtube.com/watch?v=szqQNtUpdns\"\n",
    "YT_ID = YOUTUBE_URL.split(\"=\")[-1].split(\"?\")[0]\n",
    "\n",
    "print(f\"Targeting Video ID: {YT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading",
   "metadata": {},
   "source": [
    "## 2. Download & Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_parse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download source materials\n",
    "transcript_filepath = youtube_client.download_transcript(YT_ID)\n",
    "chat_filepath = youtube_client.download_live_chat(YT_ID)\n",
    "\n",
    "# Parse DataFrames\n",
    "parsed_transcript_df = pd.DataFrame()\n",
    "parsed_chat_df = pd.DataFrame()\n",
    "\n",
    "if transcript_filepath:\n",
    "    parsed_transcript_df = parsers.parse_transcript_vtt(transcript_filepath)\n",
    "else:\n",
    "    print(f\"No transcript available for: {YT_ID}\")\n",
    "\n",
    "if chat_filepath:\n",
    "    parsed_chat_df = parsers.parse_live_chat_json(chat_filepath)\n",
    "else:\n",
    "    print(f\"No live chat available for: {YT_ID}\")\n",
    "\n",
    "# Display Info\n",
    "print(\"\\n--- Transcript Info ---\")\n",
    "parsed_transcript_df.info()\n",
    "print(\"\\n--- Chat Info ---\")\n",
    "parsed_chat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693dab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = [\n",
    "    (\"Faze\", \"Phase\")\n",
    "]\n",
    "\n",
    "for original, replace in replace_dict:\n",
    "    parsed_transcript_df['text'] = parsed_transcript_df['text'].str.replace(original, replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_peaks",
   "metadata": {},
   "source": [
    "## 3. General Activity Analysis\n",
    "Identify moments where chat volume spikes significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not parsed_chat_df.empty:\n",
    "    # Aggregate by minute\n",
    "    messages_per_minute = parsed_chat_df.groupby(\"minute\").size().rename(\"message_count\").reset_index()\n",
    "    \n",
    "    # Peak Detection\n",
    "    counts = messages_per_minute[\"message_count\"].values\n",
    "    # Peak threshold: Mean + 2 Standard Deviations\n",
    "    threshold = counts.mean() + (2 * counts.std())\n",
    "    peaks, _ = find_peaks(counts, height=threshold)\n",
    "    peak_minutes = messages_per_minute.loc[peaks, \"minute\"].tolist()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    bars = plt.bar(messages_per_minute['minute'], messages_per_minute['message_count'], \n",
    "                   color='skyblue', label='Messages/Min')\n",
    "\n",
    "    # Highlight spikes\n",
    "    for minute in peak_minutes:\n",
    "        idx = messages_per_minute[messages_per_minute['minute'] == minute].index\n",
    "        if not idx.empty:\n",
    "            bars[idx[0]].set_color('salmon')\n",
    "\n",
    "    plt.title(f'Chat Volume per Minute (Spikes > {int(threshold)} msgs)', fontsize=14)\n",
    "    plt.xlabel('Minute Offset')\n",
    "    plt.ylabel('Message Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show context for peaks using the new helper function\n",
    "    if not parsed_transcript_df.empty and peak_minutes:\n",
    "        print(f\"\\n--- CONTEXT FOR HIGH TRAFFIC MOMENTS (>{int(threshold)} msgs) ---\")\n",
    "        transcript_context = extract_transcript_by_minutes(parsed_transcript_df, peak_minutes)\n",
    "        \n",
    "        for m in sorted(peak_minutes):\n",
    "            print(f\"\\n[Minute {m}] {messages_per_minute.loc[messages_per_minute['minute'] == m, 'message_count'].values[0]} msgs\")\n",
    "            print(f\"{textwrap.fill(transcript_context[m], width=80, initial_indent='  ', subsequent_indent='  ')}\")\n",
    "else:\n",
    "    print(\"Skipping analysis: No chat data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_users",
   "metadata": {},
   "source": [
    "## 4. User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gini_lorenz",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not parsed_chat_df.empty:\n",
    "    # Calculate Counts\n",
    "    user_counts = parsed_chat_df[\"author_name\"].value_counts().reset_index()\n",
    "    user_counts.columns = [\"author_name\", \"msg_count\"]\n",
    "\n",
    "    # --- Gini & Lorenz Curve ---\n",
    "    counts = user_counts[\"msg_count\"].values\n",
    "    counts_sorted = np.sort(counts)\n",
    "    n = len(counts_sorted)\n",
    "    \n",
    "    # Lorenz calc\n",
    "    cum_counts = np.cumsum(counts_sorted)\n",
    "    normalized_cum_counts = cum_counts / cum_counts[-1]\n",
    "    \n",
    "    # Gini calc\n",
    "    # Area under Lorenz curve is sum(cum_counts) / total_sum / n roughly, \n",
    "    # but standard formula is (2 * Area_between_line_equality_and_lorenz)\n",
    "    gini = (n + 1 - 2 * np.sum(cum_counts) / cum_counts[-1]) / n\n",
    "\n",
    "    x_axis = np.linspace(0, 1, len(normalized_cum_counts))\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(x_axis, normalized_cum_counts, label=f'Gini: {gini:.3f}', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Equality')\n",
    "    plt.title('Lorenz Curve (Chat Inequality)')\n",
    "    plt.xlabel('Cumulative % of Users')\n",
    "    plt.ylabel('Cumulative % of Messages')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Top 5 Chatters:\\n{user_counts.head(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keyword_deep_dive",
   "metadata": {},
   "source": [
    "## 5. Keyword & Transcript Deep Dive (New Feature)\n",
    "Define a list of keywords to see where they appear in chat, and cross-reference with what was being said in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keyword_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER CONFIG ---\n",
    "TARGET_KEYWORDS = [\"lmao\", \"lol\", \"wow\", \"gg\", \"kekw\", \"wtf\", \"fuck\"] \n",
    "MIN_HIT_THRESHOLD = 5 # Only show subtitles if keywords appear > this many times in a minute\n",
    "# ------------------\n",
    "\n",
    "if not parsed_chat_df.empty and not parsed_transcript_df.empty:\n",
    "    \n",
    "    # 1. Filter chat for keywords\n",
    "    keyword_hits = []\n",
    "    \n",
    "    print(\"Scanning chat for keywords...\")\n",
    "    for idx, row in parsed_chat_df.iterrows():\n",
    "        msg_lower = str(row['message']).lower()\n",
    "        minute = row['minute']\n",
    "        \n",
    "        for kw in TARGET_KEYWORDS:\n",
    "            if kw in msg_lower:\n",
    "                keyword_hits.append({\n",
    "                    'minute': minute,\n",
    "                    'keyword': kw\n",
    "                })\n",
    "    \n",
    "    df_hits = pd.DataFrame(keyword_hits)\n",
    "    \n",
    "    if not df_hits.empty:\n",
    "        # 2. Pivot for Stacked Bar Chart\n",
    "        pivot_df = df_hits.groupby(['minute', 'keyword']).size().unstack(fill_value=0)\n",
    "        \n",
    "        all_minutes = range(int(parsed_chat_df['minute'].min()), int(parsed_chat_df['minute'].max()) + 1)\n",
    "        pivot_df = pivot_df.reindex(all_minutes, fill_value=0)\n",
    "\n",
    "        # 3. Plot\n",
    "        ax = pivot_df.plot(kind='bar', stacked=True, figsize=(18, 8), width=1.0, colormap='viridis')\n",
    "        \n",
    "        ticks = ax.xaxis.get_ticklocs()\n",
    "        ticklabels = [l.get_text() for l in ax.xaxis.get_ticklabels()]\n",
    "        ax.xaxis.set_ticks(ticks[::10])\n",
    "        ax.xaxis.set_ticklabels(ticklabels[::10], rotation=0)\n",
    "        \n",
    "        plt.title(f'Keyword Frequency per Minute: {TARGET_KEYWORDS}', fontsize=16)\n",
    "        plt.xlabel('Minute Offset')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend(title='Keywords')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4. Context Extraction (Subtitles)\n",
    "        minutes_meeting_threshold_mask = (pivot_df >= MIN_HIT_THRESHOLD).any(axis=1)\n",
    "        \n",
    "        \n",
    "        # Sort the significant ones by activity\n",
    "        top_minutes = pivot_df[minutes_meeting_threshold_mask].index.tolist()\n",
    "        \n",
    "        if top_minutes:\n",
    "            print(f\"\\n--- TRANSCRIPT CONTEXT FOR TOP SPIKES (Threshold: {MIN_HIT_THRESHOLD}+ hits) ---\")\n",
    "            \n",
    "            # --- USE NEW HELPER FUNCTION HERE ---\n",
    "            transcript_context = extract_transcript_by_minutes(parsed_transcript_df, top_minutes)\n",
    "            \n",
    "            for m in sorted(top_minutes):\n",
    "                breakdown = dict(pivot_df.loc[m][pivot_df.loc[m] > 0])\n",
    "                print(f\"\\n[Minute {m}] Keywords: {breakdown}\")\n",
    "                print(f\"{textwrap.fill(transcript_context[m], width=80, initial_indent='  ', subsequent_indent='  ')}\")\n",
    "        else:\n",
    "            print(f\"\\nNo minutes found with more than {MIN_HIT_THRESHOLD} keyword hits. Try lowering the threshold.\")\n",
    "                \n",
    "    else:\n",
    "        print(\"No matches found for the provided keywords.\")\n",
    "else:\n",
    "    print(\"Chat or Transcript data missing, cannot run Deep Dive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "if not parsed_chat_df.empty:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # 1. Calculate sentiment for each chat message\n",
    "    parsed_chat_df['sentiment'] = parsed_chat_df['message'].apply(lambda msg: analyzer.polarity_scores(msg)['compound'])\n",
    "\n",
    "    # 2. Aggregate metrics per minute\n",
    "    messages_per_minute = parsed_chat_df.groupby('minute').agg(\n",
    "        message_count=('message', 'size'),\n",
    "        avg_sentiment=('sentiment', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # 3. Calculate a highlight score\n",
    "    # We normalize message count and sentiment to combine them.\n",
    "    # A highlight is a combination of high activity and positive sentiment.\n",
    "    msg_count_norm = (messages_per_minute['message_count'] - messages_per_minute['message_count'].min()) / (messages_per_minute['message_count'].max() - messages_per_minute['message_count'].min())\n",
    "    sentiment_norm = (messages_per_minute['avg_sentiment'] - messages_per_minute['avg_sentiment'].min()) / (messages_per_minute['avg_sentiment'].max() - messages_per_minute['avg_sentiment'].min())\n",
    "\n",
    "    # Combine metrics. Adjust weighting as needed.\n",
    "    messages_per_minute['highlight_score'] = (0.7 * msg_count_norm) + (0.3 * sentiment_norm)\n",
    "\n",
    "    # 4. Find peaks in the highlight score\n",
    "    highlight_threshold = messages_per_minute['highlight_score'].mean() + 1.5 * messages_per_minute['highlight_score'].std()\n",
    "    peaks, _ = find_peaks(messages_per_minute['highlight_score'], height=highlight_threshold)\n",
    "    highlight_minutes = messages_per_minute.loc[peaks, 'minute'].tolist()\n",
    "\n",
    "    # 5. Visualization\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.plot(messages_per_minute['minute'], messages_per_minute['highlight_score'], label='Highlight Score', color='blue', zorder=2)\n",
    "    plt.scatter(messages_per_minute.loc[peaks, 'minute'], messages_per_minute.loc[peaks, 'highlight_score'], color='red', s=100, label='Detected Highlights', zorder=5, marker='*')\n",
    "    plt.axhline(y=highlight_threshold, color='gray', linestyle='--', label='Highlight Threshold')\n",
    "    plt.title('Automated Highlight Detection', fontsize=16)\n",
    "    plt.xlabel('Minute of Stream')\n",
    "    plt.ylabel('Calculated Highlight Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 6. Print context for highlights\n",
    "    if not parsed_transcript_df.empty and highlight_minutes:\n",
    "        print(f'--- CONTEXT FOR DETECTED HIGHLIGHTS ---')\n",
    "        for minute in sorted(highlight_minutes):\n",
    "            start_sec = minute * 60\n",
    "            end_sec = (minute + 1) * 60\n",
    "            mask = (parsed_transcript_df['offset_start_seconds'] >= start_sec) & (parsed_transcript_df['offset_start_seconds'] < end_sec)\n",
    "            segment = parsed_transcript_df[mask]\n",
    "            transcript_text = \" \".join(segment['text'].tolist()).strip() if not segment.empty else \"(No speech detected)\"\n",
    "            print(f'[Minute {minute}]')\n",
    "            print(f'  Streamer said: {textwrap.fill(transcript_text, width=80, initial_indent=\"    \", subsequent_indent=\"    \")}')\n",
    "else:\n",
    "    print('Chat data is not available, cannot perform highlight detection.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcript-archiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
