{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Stream Analysis Flow\n",
    "This notebook processes YouTube transcripts and live chat logs to analyze engagement, spikes, and keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Workspace Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import textwrap\n",
    "\n",
    "# Custom module\n",
    "import parsers\n",
    "import youtube_client # Ensure this file exists in your directory\n",
    "\n",
    "# Pre-download NLTK data\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Chart styling\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTIONS ---\n",
    "def extract_transcript_by_minutes(transcript_df, minutes):\n",
    "    \"\"\"\n",
    "    Retrieves subtitle text for specific minutes.\n",
    "    \n",
    "    Args:\n",
    "        transcript_df (pd.DataFrame): The dataframe containing transcript data.\n",
    "        minutes (list or int): A single minute (int) or list of minutes to retrieve.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where key is the minute (int) and value is the text (str).\n",
    "    \"\"\"\n",
    "    if isinstance(minutes, (int, float)):\n",
    "        minutes = [int(minutes)]\n",
    "        \n",
    "    results = {}\n",
    "    for m in minutes:\n",
    "        start_sec = m * 60\n",
    "        end_sec = (m + 1) * 60\n",
    "        \n",
    "        # Filter rows that start within this minute\n",
    "        mask = (transcript_df['offset_start_seconds'] >= start_sec) & \\\n",
    "               (transcript_df['offset_start_seconds'] < end_sec)\n",
    "               \n",
    "        segment = transcript_df[mask]\n",
    "        if not segment.empty:\n",
    "            # Join text and clean up extra spaces\n",
    "            text = \" \".join(segment['text'].tolist())\n",
    "            results[m] = text.strip()\n",
    "        else:\n",
    "            results[m] = \"(No speech detected)\"\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YOUTUBE_URL = \"https://www.youtube.com/watch?v=XKGVM86HLOQ\"\n",
    "YT_ID = YOUTUBE_URL.split(\"=\")[-1].split(\"?\")[0]\n",
    "\n",
    "print(f\"Targeting Video ID: {YT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading",
   "metadata": {},
   "source": [
    "## 2. Download & Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_parse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download source materials\n",
    "transcript_filepath = youtube_client.download_transcript(YT_ID)\n",
    "chat_filepath = youtube_client.download_live_chat(YT_ID)\n",
    "\n",
    "# Parse DataFrames\n",
    "parsed_transcript_df = pd.DataFrame()\n",
    "parsed_chat_df = pd.DataFrame()\n",
    "\n",
    "if transcript_filepath:\n",
    "    parsed_transcript_df = parsers.parse_transcript_vtt(transcript_filepath)\n",
    "else:\n",
    "    print(f\"No transcript available for: {YT_ID}\")\n",
    "\n",
    "if chat_filepath:\n",
    "    parsed_chat_df = parsers.parse_live_chat_json(chat_filepath)\n",
    "else:\n",
    "    print(f\"No live chat available for: {YT_ID}\")\n",
    "\n",
    "# Display Info\n",
    "print(\"\\n--- Transcript Info ---\")\n",
    "parsed_transcript_df.info()\n",
    "print(\"\\n--- Chat Info ---\")\n",
    "parsed_chat_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_peaks",
   "metadata": {},
   "source": [
    "## 3. General Activity Analysis\n",
    "Identify moments where chat volume spikes significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not parsed_chat_df.empty:\n",
    "    # Aggregate by minute\n",
    "    messages_per_minute = parsed_chat_df.groupby(\"minute\").size().rename(\"message_count\").reset_index()\n",
    "    \n",
    "    # Peak Detection\n",
    "    counts = messages_per_minute[\"message_count\"].values\n",
    "    # Peak threshold: Mean + 2 Standard Deviations\n",
    "    threshold = counts.mean() + (2 * counts.std())\n",
    "    peaks, _ = find_peaks(counts, height=threshold)\n",
    "    peak_minutes = messages_per_minute.loc[peaks, \"minute\"].tolist()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    bars = plt.bar(messages_per_minute['minute'], messages_per_minute['message_count'], \n",
    "                   color='skyblue', label='Messages/Min')\n",
    "\n",
    "    # Highlight spikes\n",
    "    for minute in peak_minutes:\n",
    "        idx = messages_per_minute[messages_per_minute['minute'] == minute].index\n",
    "        if not idx.empty:\n",
    "            bars[idx[0]].set_color('salmon')\n",
    "\n",
    "    plt.title(f'Chat Volume per Minute (Spikes > {int(threshold)} msgs)', fontsize=14)\n",
    "    plt.xlabel('Minute Offset')\n",
    "    plt.ylabel('Message Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show context for peaks using the new helper function\n",
    "    if not parsed_transcript_df.empty and peak_minutes:\n",
    "        print(f\"\\n--- CONTEXT FOR HIGH TRAFFIC MOMENTS (>{int(threshold)} msgs) ---\")\n",
    "        transcript_context = extract_transcript_by_minutes(parsed_transcript_df, peak_minutes)\n",
    "        \n",
    "        for m in sorted(peak_minutes):\n",
    "            print(f\"\\n[Minute {m}] {messages_per_minute.loc[messages_per_minute['minute'] == m, 'message_count'].values[0]} msgs\")\n",
    "            print(f\"Streamer said: {textwrap.fill(transcript_context[m], width=80, initial_indent='  ', subsequent_indent='  ')}\")\n",
    "else:\n",
    "    print(\"Skipping analysis: No chat data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_users",
   "metadata": {},
   "source": [
    "## 4. User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gini_lorenz",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not parsed_chat_df.empty:\n",
    "    # Calculate Counts\n",
    "    user_counts = parsed_chat_df[\"author_name\"].value_counts().reset_index()\n",
    "    user_counts.columns = [\"author_name\", \"msg_count\"]\n",
    "\n",
    "    # --- Gini & Lorenz Curve ---\n",
    "    counts = user_counts[\"msg_count\"].values\n",
    "    counts_sorted = np.sort(counts)\n",
    "    n = len(counts_sorted)\n",
    "    \n",
    "    # Lorenz calc\n",
    "    cum_counts = np.cumsum(counts_sorted)\n",
    "    normalized_cum_counts = cum_counts / cum_counts[-1]\n",
    "    \n",
    "    # Gini calc\n",
    "    # Area under Lorenz curve is sum(cum_counts) / total_sum / n roughly, \n",
    "    # but standard formula is (2 * Area_between_line_equality_and_lorenz)\n",
    "    gini = (n + 1 - 2 * np.sum(cum_counts) / cum_counts[-1]) / n\n",
    "\n",
    "    x_axis = np.linspace(0, 1, len(normalized_cum_counts))\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(x_axis, normalized_cum_counts, label=f'Gini: {gini:.3f}', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Equality')\n",
    "    plt.title('Lorenz Curve (Chat Inequality)')\n",
    "    plt.xlabel('Cumulative % of Users')\n",
    "    plt.ylabel('Cumulative % of Messages')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Top 5 Chatters:\\n{user_counts.head(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keyword_deep_dive",
   "metadata": {},
   "source": [
    "## 5. Keyword & Transcript Deep Dive (New Feature)\n",
    "Define a list of keywords to see where they appear in chat, and cross-reference with what was being said in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keyword_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER CONFIG ---\n",
    "TARGET_KEYWORDS = [\"lmao\", \"lol\", \"wow\", \"gg\", \"kekw\"] \n",
    "MIN_HIT_THRESHOLD = 5 # Only show subtitles if keywords appear > this many times in a minute\n",
    "# ------------------\n",
    "\n",
    "if not parsed_chat_df.empty and not parsed_transcript_df.empty:\n",
    "    \n",
    "    # 1. Filter chat for keywords\n",
    "    keyword_hits = []\n",
    "    \n",
    "    print(\"Scanning chat for keywords...\")\n",
    "    for idx, row in parsed_chat_df.iterrows():\n",
    "        msg_lower = str(row['message']).lower()\n",
    "        minute = row['minute']\n",
    "        \n",
    "        for kw in TARGET_KEYWORDS:\n",
    "            if kw in msg_lower:\n",
    "                keyword_hits.append({\n",
    "                    'minute': minute,\n",
    "                    'keyword': kw\n",
    "                })\n",
    "    \n",
    "    df_hits = pd.DataFrame(keyword_hits)\n",
    "    \n",
    "    if not df_hits.empty:\n",
    "        # 2. Pivot for Stacked Bar Chart\n",
    "        pivot_df = df_hits.groupby(['minute', 'keyword']).size().unstack(fill_value=0)\n",
    "        \n",
    "        all_minutes = range(int(parsed_chat_df['minute'].min()), int(parsed_chat_df['minute'].max()) + 1)\n",
    "        pivot_df = pivot_df.reindex(all_minutes, fill_value=0)\n",
    "\n",
    "        # 3. Plot\n",
    "        ax = pivot_df.plot(kind='bar', stacked=True, figsize=(18, 8), width=1.0, colormap='viridis')\n",
    "        \n",
    "        ticks = ax.xaxis.get_ticklocs()\n",
    "        ticklabels = [l.get_text() for l in ax.xaxis.get_ticklabels()]\n",
    "        ax.xaxis.set_ticks(ticks[::10])\n",
    "        ax.xaxis.set_ticklabels(ticklabels[::10], rotation=0)\n",
    "        \n",
    "        plt.title(f'Keyword Frequency per Minute: {TARGET_KEYWORDS}', fontsize=16)\n",
    "        plt.xlabel('Minute Offset')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend(title='Keywords')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4. Context Extraction (Subtitles)\n",
    "        # Calculate total hits per minute\n",
    "        total_hits_series = pivot_df.sum(axis=1)\n",
    "        \n",
    "        # Filter: Keep only minutes >= threshold\n",
    "        significant_minutes = total_hits_series[total_hits_series >= MIN_HIT_THRESHOLD]\n",
    "        \n",
    "        # Sort the significant ones by activity\n",
    "        top_minutes = significant_minutes.sort_values(ascending=False).head(3).index.tolist()\n",
    "        \n",
    "        if top_minutes:\n",
    "            print(f\"\\n--- TRANSCRIPT CONTEXT FOR TOP SPIKES (Threshold: {MIN_HIT_THRESHOLD}+ hits) ---\")\n",
    "            \n",
    "            # --- USE NEW HELPER FUNCTION HERE ---\n",
    "            transcript_context = extract_transcript_by_minutes(parsed_transcript_df, top_minutes)\n",
    "            \n",
    "            for m in sorted(top_minutes):\n",
    "                hits_in_minute = total_hits_series.loc[m]\n",
    "                breakdown = dict(pivot_df.loc[m][pivot_df.loc[m] > 0])\n",
    "                print(f\"\\n[Minute {m}] Total Hits: {hits_in_minute} | Keywords: {breakdown}\")\n",
    "                print(f\"{textwrap.fill(transcript_context[m], width=80, initial_indent='  ', subsequent_indent='  ')}\")\n",
    "        else:\n",
    "            print(f\"\\nNo minutes found with more than {MIN_HIT_THRESHOLD} keyword hits. Try lowering the threshold.\")\n",
    "                \n",
    "    else:\n",
    "        print(\"No matches found for the provided keywords.\")\n",
    "else:\n",
    "    print(\"Chat or Transcript data missing, cannot run Deep Dive.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
